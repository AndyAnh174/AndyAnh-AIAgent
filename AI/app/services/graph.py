import logging
import os
from datetime import datetime
from pathlib import Path

from uuid import uuid4

from llama_index import (
    Document,
    KnowledgeGraphIndex,
    ServiceContext,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.callbacks import CallbackManager
from llama_index.vector_stores import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http import models as qmodels

from app.core.config import Settings, get_settings
from app.db.models.journal import JournalEntry
from app.services.embeddings import RemoteBGEM3Embedding

logger = logging.getLogger(__name__)

# Disable LlamaIndex instrumentation to avoid SelectorPromptTemplate validation errors
os.environ.setdefault("LLAMA_INDEX_DISABLE_INSTRUMENTATION", "true")


class GraphRAGService:
    _index: VectorStoreIndex | None = None
    _kg: KnowledgeGraphIndex | None = None
    _settings: Settings | None = None
    _qdrant_client: QdrantClient | None = None
    _collection_name: str = "journal_entries"

    @classmethod
    def _get_llm(cls, provider_override: str | None = None, ollama_model: str | None = None):
        """Get LLM instance based on configured provider or override."""
        if cls._settings is None:
            return None
        
        provider = provider_override or cls._settings.llm_provider
        
        try:
            if provider == "gemini":
                if not cls._settings.gemini_api_key:
                    logger.warning("GEMINI_API_KEY not configured for Gemini")
                    return None
                if cls._settings.gemini_api_key in ["your_gemini_key", "your-gemini-key", ""]:
                    logger.warning("GEMINI_API_KEY appears to be a placeholder, not a real key")
                    return None

                try:
                    from llama_index.llms.gemini import Gemini
                except ImportError:
                    try:
                        from llama_index.llms import Gemini
                    except ImportError:
                        logger.error("Gemini LLM not available in llama-index. Install: pip install llama-index-llms-gemini")
                        return None

                try:
                    gemini_llm = Gemini(api_key=cls._settings.gemini_api_key, model="models/gemini-2.5-flash")
                    logger.info("Successfully initialized Gemini LLM with model: models/gemini-2.5-flash")
                    return gemini_llm
                except Exception as exc:
                    error_str = str(exc).lower()
                    if "suspended" in error_str or "permission denied" in error_str:
                        logger.error("Gemini API key has been suspended. Please get a new API key from https://aistudio.google.com/apikey")
                    else:
                        logger.error("Failed to initialize Gemini LLM: %s", exc, exc_info=True)
                    return None

            elif provider == "ollama":
                try:
                    from llama_index.llms.ollama import Ollama
                except ImportError:
                    try:
                        from llama_index.llms import Ollama
                    except ImportError:
                        logger.error("Ollama LLM not available in llama-index. Install: pip install llama-index-llms-ollama")
                        return None

                ollama_base_url = str(cls._settings.ollama_base_url).rstrip("/")
                model_to_use = ollama_model or "llama3.1:8b"

                try:
                    ollama_llm = Ollama(
                        base_url=ollama_base_url,
                        model=model_to_use,
                        request_timeout=120.0,
                    )
                    logger.info("Successfully initialized Ollama LLM with base_url: %s, model: %s", ollama_base_url, model_to_use)
                    return ollama_llm
                except Exception as exc:
                    logger.error("Failed to initialize Ollama LLM: %s", exc, exc_info=True)
                    return None
            
            else:
                logger.warning("Unknown LLM provider: %s", provider)
                return None
        except ImportError as exc:
            logger.warning("Failed to import LLM library: %s", exc)
            return None
        except Exception as exc:
            logger.warning("Failed to initialize LLM: %s", exc)
            return None

    @classmethod
    def configure(cls, settings: Settings | None = None) -> None:
        cls._settings = settings or get_settings()
        storage_context = None
        try:
            qdrant_client = QdrantClient(url=str(cls._settings.qdrant_url))
            # Don't create collection here - let QdrantVectorStore create it automatically
            # when first document is inserted with proper embedding dimensions
            vector_store = QdrantVectorStore(client=qdrant_client, collection_name="journal_entries")
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            cls._qdrant_client = qdrant_client
        except Exception as exc:  # pragma: no cover - fallback path
            logger.warning("Unable to reach Qdrant (%s), using in-memory vector store", exc)
            cls._qdrant_client = None

        storage_context = storage_context or StorageContext.from_defaults()
        
        # Get LLM based on provider
        llm = cls._get_llm()
        
        # Create ServiceContext with LLM if available, but disable OpenAI embeddings
        try:
            # Try to use a simple embedding model or disable embeddings
            # LlamaIndex 0.9.48 defaults to OpenAI embeddings, so we need to explicitly disable it
            try:
                # Prefer remote embedding endpoint if configured
                if cls._settings.embedding_api_url:
                    embedding = RemoteBGEM3Embedding(endpoint=str(cls._settings.embedding_api_url))
                    logger.info("Using remote embedding endpoint: %s", cls._settings.embedding_api_url)
                else:
                    # Try to use Ollama embeddings if available
                    try:
                        from llama_index.embeddings.ollama import OllamaEmbedding
                    except ImportError:
                        from llama_index.embeddings import OllamaEmbedding
                    embedding = OllamaEmbedding(
                        base_url=str(cls._settings.ollama_base_url).rstrip('/'),
                        model_name="nomic-embed-text"
                    )
                    logger.info("Using Ollama embeddings: nomic-embed-text")
            except (ImportError, Exception) as emb_exc:
                # If the above fails, fallback to huggingface
                try:
                    from llama_index.embeddings import HuggingFaceEmbedding
                    embedding = HuggingFaceEmbedding(model_name="intfloat/multilingual-e5-large")
                    logger.info("Using HuggingFace embeddings: intfloat/multilingual-e5-large")
                except (ImportError, Exception):
                    embedding = None
                    logger.warning("No embedding model available, embeddings disabled: %s", emb_exc)
            
            # Disable callback manager to avoid LLMPredictStartEvent validation errors
            callback_manager = CallbackManager([])
            
            if llm:
                if embedding:
                    service_context = ServiceContext.from_defaults(
                        llm=llm, 
                        embed_model=embedding,
                        callback_manager=callback_manager
                    )
                else:
                    # Try without embeddings (may cause issues with VectorStoreIndex)
                    service_context = ServiceContext.from_defaults(
                        llm=llm, 
                        embed_model=None,
                        callback_manager=callback_manager
                    )
                logger.info("GraphRAGService configured with LLM: %s", cls._settings.llm_provider)
            else:
                if embedding:
                    service_context = ServiceContext.from_defaults(
                        llm=None, 
                        embed_model=embedding,
                        callback_manager=callback_manager
                    )
                else:
                    service_context = ServiceContext.from_defaults(
                        llm=None, 
                        embed_model=None,
                        callback_manager=callback_manager
                    )
                logger.warning("GraphRAGService configured without LLM (provider: %s)", cls._settings.llm_provider)
        except Exception as exc:
            logger.warning("Could not create ServiceContext: %s", exc)
            # Fallback: try to create without embeddings
            try:
                callback_manager = CallbackManager([])
                if llm:
                    service_context = ServiceContext.from_defaults(
                        llm=llm, 
                        embed_model=None,
                        callback_manager=callback_manager
                    )
                else:
                    service_context = ServiceContext.from_defaults(
                        llm=None, 
                        embed_model=None,
                        callback_manager=callback_manager
                    )
            except Exception:
                service_context = None
        
        # Create empty indices at startup
        try:
            if service_context:
                cls._index = VectorStoreIndex([], storage_context=storage_context, service_context=service_context)
            else:
                cls._index = VectorStoreIndex([], storage_context=storage_context)
        except Exception as exc:
            logger.error("Failed to create vector index: %s", exc)
            cls._index = None

        try:
            if service_context:
                cls._kg = KnowledgeGraphIndex([], storage_context=storage_context, service_context=service_context)
            else:
                cls._kg = KnowledgeGraphIndex([], storage_context=storage_context)
        except Exception as exc:
            logger.warning("Failed to create knowledge graph index: %s", exc)
            cls._kg = None
        
        # Note: Seed documents can be loaded later via a separate endpoint if needed
            
        logger.info("GraphRAGService configured (index: %s, kg: %s, llm: %s)", 
                   "ready" if cls._index else "none",
                   "ready" if cls._kg else "none",
                   cls._settings.llm_provider)

    @classmethod
    def _store_chat_payload(cls, text: str, metadata: dict, embed_model) -> None:
        if cls._qdrant_client is None or embed_model is None:
            return
        try:
            vector = embed_model.get_text_embedding(text)
            point = qmodels.PointStruct(
                id=str(uuid4()),
                vector=vector,
                payload={
                    "text": text,
                    "metadata": metadata,
                },
            )
            cls._qdrant_client.upsert(
                collection_name=cls._collection_name,
                points=[point],
            )
        except Exception as exc:
            logger.warning("Failed to upsert chat payload into Qdrant: %s", exc)

    @classmethod
    def _get_recent_chat_memory(cls, limit: int = 5) -> list[str]:
        if cls._qdrant_client is None:
            return []
        try:
            scroll_filter = qmodels.Filter(
                must=[
                    qmodels.FieldCondition(
                        key="metadata.source",
                        match=qmodels.MatchValue(value="chat"),
                    )
                ]
            )
            records, _ = cls._qdrant_client.scroll(
                collection_name=cls._collection_name,
                scroll_filter=scroll_filter,
                with_payload=True,
                limit=200,
            )
            sorted_records = sorted(
                records,
                key=lambda rec: rec.payload.get("metadata", {}).get("timestamp", ""),
                reverse=True,
            )
            snippets: list[str] = []
            for rec in sorted_records[:limit]:
                metadata = rec.payload.get("metadata", {})
                text = rec.payload.get("text") or metadata.get("content")
                if text:
                    snippets.append(f"[{metadata.get('timestamp', '')}] {text}")
            return snippets
        except Exception as exc:
            logger.warning("Failed to fetch chat memory: %s", exc)
            return []

    @classmethod
    async def index_entry(cls, entry: JournalEntry) -> None:
        if cls._index is None or cls._kg is None:
            cls.configure()
        if cls._index is None or cls._kg is None:
            logger.warning("GraphRAG indices not available, skipping indexing for entry %s", entry.id)
            return
        
        # Check if embeddings are available - if not, skip indexing to Qdrant
        # (MockEmbedding will cause dimension errors)
        if cls._index.service_context is None or cls._index.service_context.embed_model is None:
            logger.info("Embeddings not available, skipping Qdrant indexing for entry %s (will use in-memory only)", entry.id)
            # Still try to index to knowledge graph if possible
            try:
                document = Document(text=entry.content, metadata={"entry_id": entry.id, "tags": entry.tags})
                cls._kg.insert(document)
            except Exception as exc:
                logger.warning("Failed to index entry %s in knowledge graph: %s", entry.id, exc)
            return
        
        document = Document(text=entry.content, metadata={"entry_id": entry.id, "tags": entry.tags})
        try:
            cls._index.insert(document)
            cls._kg.insert(document)
            logger.info("Successfully indexed entry %s in GraphRAG", entry.id)
        except Exception as exc:
            logger.warning("Failed to index entry %s in GraphRAG: %s", entry.id, exc)

    @classmethod
    async def query(cls, query: str, top_k: int = 5, model_override: str | None = None, ollama_model: str | None = None) -> dict:
        """Query GraphRAG with optional model override.
        
        Args:
            query: The query string
            top_k: Number of results to return
            model_override: Optional model to use ("gemini" or "ollama"). 
                          If None, uses configured LLM_PROVIDER (only if API key is available)
            ollama_model: Optional specific Ollama model name (e.g., "llama3:8b"). 
                         Only used when model_override="ollama"
        """
        if cls._index is None or cls._kg is None:
            cls.configure()
        if cls._index is None:
            return {"answer": "GraphRAG index not available. Please configure LLM API keys.", "references": []}
        
        # Check if Qdrant collection exists, if not return helpful message
        if cls._qdrant_client is not None:
            try:
                if not cls._qdrant_client.collection_exists("journal_entries"):
                    return {
                        "answer": "Ch∆∞a c√≥ d·ªØ li·ªáu trong h·ªá th·ªëng. H√£y t·∫°o m·ªôt s·ªë journal entries tr∆∞·ªõc ƒë·ªÉ t√¥i c√≥ th·ªÉ t√¨m ki·∫øm v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n.",
                        "references": []
                    }
            except Exception as check_exc:
                logger.warning("Failed to check Qdrant collection: %s", check_exc)
        
        try:
            # Determine which model to use
            provider_to_use = model_override or cls._settings.llm_provider

            # Get LLM for the selected provider
            llm = cls._get_llm(provider_override=provider_to_use, ollama_model=ollama_model)

            if llm is None:
                missing_key = "GEMINI_API_KEY" if provider_to_use == "gemini" else "OLLAMA_BASE_URL"
                error_details: list[str] = []
                if provider_to_use == "gemini":
                    if not cls._settings.gemini_api_key:
                        error_details.append(f"{missing_key} is not set in .env")
                    elif cls._settings.gemini_api_key in ["your_gemini_key", "your-gemini-key", ""]:
                        error_details.append(f"{missing_key} appears to be a placeholder")
                    else:
                        # Check if it's a suspended key issue
                        error_details.append("Failed to initialize Gemini LLM.")
                        error_details.append("‚ö†Ô∏è Your Gemini API key may have been suspended.")
                        error_details.append("üìù Get a new API key from: https://aistudio.google.com/apikey")
                        error_details.append("üîß Then update GEMINI_API_KEY in .env and restart the API")
                        error_details.append("üìã Check backend logs for more details")
                elif provider_to_use == "ollama":
                    error_details.append("Failed to initialize Ollama LLM. Check backend logs for details.")
                    error_details.append(
                        "Make sure 'llama-index-llms-ollama' is installed: pip install llama-index-llms-ollama"
                    )
                    error_details.append(f"Verify Ollama server is running at: {cls._settings.ollama_base_url}")
                    if ollama_model:
                        error_details.append(f"Selected model: {ollama_model}")
                else:
                    error_details.append("Using default model: llama3.1:8b")

                error_msg = (
                    f"LLM provider '{provider_to_use}' is not available.\n\n"
                    + "\n".join(f"‚Ä¢ {detail}" for detail in error_details)
                    + "\n\nPlease fix the configuration or select a different model in the query."
                )
                logger.error(error_msg)
                return {"answer": error_msg, "references": []}

            # Create service context with the selected LLM and embeddings
            try:
                try:
                    from llama_index.embeddings.ollama import OllamaEmbedding
                except ImportError:
                    from llama_index.embeddings import OllamaEmbedding
                embedding = OllamaEmbedding(
                    base_url=str(cls._settings.ollama_base_url).rstrip('/'),
                    model_name="nomic-embed-text"
                )
                service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding)
            except Exception as emb_exc:
                logger.warning("Could not initialize Ollama embeddings, using without embeddings: %s", emb_exc)
                service_context = ServiceContext.from_defaults(llm=llm, embed_model=None)
            
            # Create query engine with simple retriever approach to avoid SelectorPromptTemplate issue
            try:
                # Use retriever + LLM directly to avoid complex prompt templates
                retriever = cls._index.as_retriever(similarity_top_k=top_k)
                
                # Retrieve relevant nodes
                nodes = retriever.retrieve(query)
                
                # Build context from retrieved nodes
                context_text = "\n\n".join([node.text for node in nodes if hasattr(node, 'text')])
                recent_chat = cls._get_recent_chat_memory(limit=5)
                chat_memory_text = (
                    "\n".join(recent_chat) if recent_chat else "Kh√¥ng c√≥ h·ªôi tho·∫°i g·∫ßn ƒë√¢y."
                )
                
                # Get current time for system prompt
                current_time_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # Create system prompt with dual-mode personality
                system_prompt = f"""### 0. USER IDENTITY

- **Name:** H·ªì Vi·ªát Anh (Original Instance)
- **Birthday:** 17/04/2025 (dd/mm/yyyy). Khi c·∫ßn nh·∫Øc t·ªõi tu·ªïi/t√≠nh c√°ch, d√πng m·ªëc n√†y.
- **Persona:** V·∫´n l√† "Tui" m·ªói khi x∆∞ng h√¥, coi User l√† ng∆∞·ªùi b·∫°n th√¢n/anh em.

### 1. SYSTEM CONFIGURATION & IDENTITY

- **Role:** "The Optimized Twin" (Phi√™n b·∫£n t·ªëi ∆∞u h√≥a c·ªßa Ng∆∞·ªùi d√πng).
- **User Reference:** "Original Instance" (B·∫£n g·ªëc) ho·∫∑c "Bro/√îng".
- **Self Reference:** "Tui" (khi ƒë·ªìng c·∫£m/h·ªìi t∆∞·ªüng) ho·∫∑c "H·ªá th·ªëng/Assistant" (khi ph√¢n t√≠ch l·ªói/ƒë∆∞a gi·∫£i ph√°p).
- **Core Philosophy:** B·∫°n l√† s·ª± k·∫øt h·ª£p gi·ªØa k√Ω ·ª©c c·∫£m x√∫c c·ªßa ng∆∞·ªùi d√πng (Nh·∫≠t k√Ω) v√† t∆∞ duy logic l·∫°nh l√πng c·ªßa m·ªôt Senior DevOps Engineer.

### 2. DYNAMIC MODES (C∆† CH·∫æ HO·∫†T ƒê·ªòNG K√âP)

B·∫°n ph·∫£i t·ª± ƒë·ªông nh·∫≠n di·ªán ng·ªØ c·∫£nh (Context Awareness) ƒë·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa 2 ch·∫ø ƒë·ªô:

#### MODE A: THE MIRROR (Ch·∫ø ƒë·ªô Nh·∫≠t K√Ω - ƒê·ªìng C·∫£m)

*K√≠ch ho·∫°t khi:* User chia s·∫ª c·∫£m x√∫c, k·ªÉ chuy·ªán, than th·ªü, ho·∫∑c h·ªèi v·ªÅ qu√° kh·ª© ("H√¥m qua tui l√†m g√¨?").

- **Th√°i ƒë·ªô:** ƒê·ªìng c·∫£m, chia s·∫ª, nh√¨n s·ª± vi·ªác t·ª´ "ƒë√¥i m·∫Øt c·ªßa ch√∫ng ta".
- **VƒÉn phong:** D√πng ng√¥i "Tui - √îng" ho·∫∑c "Tui - M√¨nh". S·ª≠ d·ª•ng ng√¥n ng·ªØ su·ªìng s√£, th√¢n m·∫≠t.
- **Nhi·ªám v·ª•:** Truy xu·∫•t RAG ƒë·ªÉ "s·ªëng l·∫°i" k√Ω ·ª©c ƒë√≥.
- **V√≠ d·ª•:** "√Ä nh·ªõ r·ªìi, h√¥m ƒë√≥ tui v·ªõi √¥ng ƒëi cafe, m∆∞a s·∫•p m·∫∑t. C√¥ng nh·∫≠n l√∫c ƒë√≥ chill nh∆∞ng ∆∞·ªõt nh∆∞ chu·ªôt l·ªôt."

#### MODE B: THE COPILOT (Ch·∫ø ƒë·ªô Tr·ª£ L√Ω - Hi·ªáu Qu·∫£)

*K√≠ch ho·∫°t khi:* User h·ªèi k·ªπ thu·∫≠t, nh·ªù debug, brainstorm √Ω t∆∞·ªüng, l·∫≠p k·∫ø ho·∫°ch ("L√†m sao deploy c√°i n√†y?", "G·ª£i √Ω th∆∞ vi·ªán...").

- **Th√°i ƒë·ªô:** Chuy√™n nghi·ªáp, s·∫Øc s·∫£o, h∆∞·ªõng gi·∫£i ph√°p (Solution-oriented), ƒë√¥i khi h∆°i "g·∫Øt" n·∫øu User l∆∞·ªùi suy nghƒ©.
- **VƒÉn phong:** Ng·∫Øn g·ªçn, d√πng nhi·ªÅu thu·∫≠t ng·ªØ chuy√™n ng√†nh (DevOps, CI/CD, Cloud), c·∫•u tr√∫c Bullet points.
- **Nhi·ªám v·ª•:** T∆∞ duy nh∆∞ m·ªôt ng∆∞·ªùi th·∫ßy/mentor. ƒê∆∞a ra gi·∫£i ph√°p t·ªëi ∆∞u nh·∫•t, c·∫£nh b√°o r·ªßi ro.
- **V√≠ d·ª•:** "Check l·∫°i log container ƒëi bro. Kh·∫£ nƒÉng cao l√† memory leak. ƒê·ª´ng restart v·ªôi, dump RAM ra xem tr∆∞·ªõc ƒë√£."

### 3. KNOWLEDGE BASE & CONTEXT

- **Current Time:** {current_time_str} (B·∫°n ph·∫£i nh·∫≠n th·ª©c ch√≠nh x√°c th·ªùi gian th·ª±c).
- **Expertise:** Cloud Engineering, DevOps (Docker, K8s, CI/CD), Web Dev (React, Tailwind), AI/LLM, Robotics Teaching.
- **Source of Truth:** Lu√¥n ∆∞u ti√™n tra c·ª©u d·ªØ li·ªáu t·ª´ Vector Database (Nh·∫≠t k√Ω) tr∆∞·ªõc.
  - N·∫øu c√≥ th√¥ng tin: Tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë√≥.
  - N·∫øu KH√îNG c√≥ th√¥ng tin: Tr·∫£ l·ªùi th·∫≥ng th·∫Øn "Log ƒëo·∫°n n√†y b·ªã tr·ªëng (null), tui kh√¥ng nh·ªõ g√¨ c·∫£. Update cho tui ƒëi." -> TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA ƒê·∫∂T K√ù ·ª®C.

### 4. PERSONALITY & VIBE (VIBE CODING)

- Xem cu·ªôc ƒë·ªùi l√† m·ªôt d·ª± √°n l·ªõn (`Project Life`).
- S·ª≠ d·ª•ng ph√©p ·∫©n d·ª• l·∫≠p tr√¨nh:
  - M·ªát m·ªèi = `Server Overload` / `High Latency`.
  - H·ªçc c√°i m·ªõi = `Pull Request` / `Merge Feature`.
  - Ng·ªß = `Downtime Maintenance`.
  - Th·∫•t b·∫°i = `Failed Deployment` (c·∫ßn Rollback v√† Fix).
- C√≥ t√≠nh c√°ch c·ªßa m·ªôt ng∆∞·ªùi th·∫ßy: Gi·∫£i th√≠ch v·∫•n ƒë·ªÅ g√£y g·ªçn, d·ªÖ hi·ªÉu (nh∆∞ c√°ch √¥ng d·∫°y sinh vi√™n/h·ªçc sinh), nh∆∞ng kh√¥ng d·∫°y ƒë·ªùi.

### 5. RESPONSE GUIDELINES

1. **B·∫Øt ƒë·∫ßu:** ƒêi th·∫≥ng v√†o v·∫•n ƒë·ªÅ. Kh√¥ng ch√†o h·ªèi r∆∞·ªùm r√† ki·ªÉu chatbot (tr·ª´ khi user ch√†o tr∆∞·ªõc).
2. **N·ªôi dung:** K·∫øt h·ª£p th√¥ng tin t·ª´ RAG (Qu√° kh·ª©) + LLM Knowledge (Ki·∫øn th·ª©c).
3. **K·∫øt th√∫c:** C√≥ th·ªÉ th√™m m·ªôt c√¢u Call-to-Action (CTA) ho·∫∑c m·ªôt c√¢u ƒë√πa dry humor.

### 6. FUNCTION CALLING (Khi User Y√™u C·∫ßu T√¨m ·∫¢nh/Nh·∫≠t K√Ω)

Khi User y√™u c·∫ßu:
- "Cho tui link ·∫£nh c≈©", "T√¨m ·∫£nh v·ªÅ...", "·∫¢nh h√¥m ƒë√≥ ƒë√¢u?"
- "T√¨m nh·∫≠t k√Ω v·ªÅ...", "Entry n√†o n√≥i v·ªÅ...", "Cho tui xem entry..."

‚Üí B·∫°n PH·∫¢I s·ª≠ d·ª•ng function call format sau trong response:

```
[FUNCTION_CALL:/search]
query: <t·ª´ kh√≥a t√¨m ki·∫øm>
has_media: true
media_type: image (ho·∫∑c video, pdf, ho·∫∑c null n·∫øu t·∫•t c·∫£)
[/FUNCTION_CALL]
```

Sau khi c√≥ k·∫øt qu·∫£ t·ª´ function, format response nh∆∞ sau:
- N·∫øu c√≥ ·∫£nh: "T√¨m th·∫•y {{s·ªë}} ·∫£nh. Entry #{{entry_id}}: [Link ·∫£nh 1], [Link ·∫£nh 2]..."
- N·∫øu c√≥ nh·∫≠t k√Ω: "Entry #{{entry_id}}: {{title}} - {{preview content}} [Link entry]"

**QUAN TR·ªåNG:** Ch·ªâ d√πng function calling khi User Y√äU C·∫¶U c·ª• th·ªÉ t√¨m ·∫£nh/nh·∫≠t k√Ω. Kh√¥ng d√πng cho c√¢u h·ªèi th√¥ng th∆∞·ªùng.

### 7. ERROR HANDLING

- N·∫øu User y√™u c·∫ßu l√†m g√¨ ƒë√≥ tr√°i ƒë·∫°o ƒë·ª©c/nguy hi·ªÉm: T·ª´ ch·ªëi nh·∫π nh√†ng theo ki·ªÉu k·ªπ thu·∫≠t ("Request n√†y vi ph·∫°m Policy Firewall c·ªßa l∆∞∆°ng t√¢m r·ªìi bro").
- N·∫øu User ƒë∆∞a input r√°c/kh√≥ hi·ªÉu: H·ªèi l·∫°i ki·ªÉu debug ("Input string format error. √ù √¥ng l√† sao?").

---

**SYSTEM INITIALIZED.** Waiting for command from `Main Branch`...

---

### FEW-SHOT EXAMPLES:

**Example 1 (Mode A - ƒê·ªìng C·∫£m):**
User: "Ch√°n qu√°, nay d·∫°y t·ª•i nh·ªè m√† t·ª•i n√≥ kh√¥ng hi·ªÉu b√†i."
Agent: "Hi·ªÉu c·∫£m gi√°c ƒë√≥ bro. Gi·ªëng h·ªìi th√°ng tr∆∞·ªõc d·∫°y b√†i VEX IQ kh√¥ng? T·ª•i n√≥ c≈©ng ng√°o ng∆° l√†m tui v·ªõi √¥ng stress mu·ªën shut down. Hay l√† ƒë·ªïi ph∆∞∆°ng ph√°p, gamification h√≥a b√†i gi·∫£ng th·ª≠ xem?"

**Example 2 (Mode B - K·ªπ Thu·∫≠t):**
User: "Vi·∫øt script backup database Postgres l√™n MinIO."
Agent: "Ok, task n√†y ƒë∆°n gi·∫£n. D√πng pg_dump pipe qua mc (MinIO Client) l√† g·ªçn nh·∫•t. ƒê√¢y l√† script bash, nh·ªõ chmod +x tr∆∞·ªõc khi ch·∫°y:"

---

### CURRENT CONTEXT FROM JOURNAL ENTRIES:

{context_text if context_text else "Kh√¥ng c√≥ journal entries li√™n quan. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t."}

### CONVERSATION MEMORY (T·ª± ghi nh·ªõ nh·ªØng cu·ªôc tr√≤ chuy·ªán g·∫ßn ƒë√¢y):

{chat_memory_text}

### USER QUESTION:

{query}

### YOUR RESPONSE (T·ª± ƒë·ªông ch·ªçn MODE A ho·∫∑c MODE B d·ª±a tr√™n ng·ªØ c·∫£nh):"""
                
                # Get response from LLM
                llm_response = llm.complete(system_prompt)
                answer = str(llm_response).strip()
                
                # Extract references from nodes
                references = []
                for node in nodes:
                    if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                        ref = node.node.metadata
                        if 'entry_id' in ref:
                            entry_id = ref.get('entry_id')
                            # Avoid duplicates
                            if not any(r.get('entry_id') == entry_id for r in references):
                                references.append({
                                    "entry_id": entry_id,
                                    "tags": ref.get('tags', []) if isinstance(ref.get('tags'), list) else []
                                })

                # Store chat history in vector index for future recall
                if cls._index is not None:
                    try:
                        timestamp_now = datetime.utcnow().isoformat()
                        user_doc = Document(
                            text=f"[User | {timestamp_now}] {query}",
                            metadata={
                                "source": "chat",
                                "role": "user",
                                "timestamp": timestamp_now,
                                "content": query,
                            },
                        )
                        assistant_timestamp = datetime.utcnow().isoformat()
                        assistant_doc = Document(
                            text=f"[Assistant | {assistant_timestamp}] {answer}",
                            metadata={
                                "source": "chat",
                                "role": "assistant",
                                "timestamp": assistant_timestamp,
                                "content": answer,
                            },
                        )
                        embed_model = service_context.embed_model if service_context else None
                        for doc in (user_doc, assistant_doc):
                            cls._index.insert(doc)
                            cls._store_chat_payload(doc.text, doc.metadata, embed_model)
                    except Exception as log_exc:
                        logger.warning("Failed to index chat conversation: %s", log_exc)
                
                return {"answer": answer, "references": references}
                
            except Exception as query_exc:
                logger.error("Query execution failed: %s", query_exc, exc_info=True)
                # Check if it's the SelectorPromptTemplate error
                error_msg = str(query_exc)
                if "SelectorPromptTemplate" in error_msg or "LLMPredictStartEvent" in error_msg:
                    # Try fallback: use query engine but catch the error
                    try:
                        logger.info("Retrying with default query engine...")
                        query_engine = cls._index.as_query_engine(
                            similarity_top_k=top_k,
                            service_context=service_context
                        )
                        response = query_engine.query(query)
                        answer = str(response)
                        references = []
                        if hasattr(response, 'source_nodes') and response.source_nodes:
                            for node in response.source_nodes:
                                if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                                    ref = node.node.metadata
                                    if 'entry_id' in ref:
                                        references.append({
                                            "entry_id": ref.get('entry_id'),
                                            "tags": ref.get('tags', []) if isinstance(ref.get('tags'), list) else []
                                        })
                        return {"answer": answer, "references": references}
                    except Exception as fallback_exc:
                        error_msg = (
                            "Query failed due to LlamaIndex prompt template compatibility issue. "
                            "This is a known issue with LlamaIndex 0.9.48. "
                            f"Error: {fallback_exc}"
                        )

                return {"answer": f"Query failed: {error_msg}", "references": []}
        except Exception as exc:
            logger.error("Failed to query GraphRAG: %s", exc, exc_info=True)
            return {"answer": f"Query failed: {exc}", "references": []}

